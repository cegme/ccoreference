\documentclass{beamer}

\usepackage[british]{babel}
\usepackage{graphicx,hyperref,ru,url}

% The title of the presentation:
%  - first a short version which is visible at the bottom of each slide;
%  - second the full title shown on the title slide;
\title[Linear-chain CRF for NLP in MADlib]{
  Linear-chain CRF for NLP in MADlib}

% The author(s) of the presentation:
%  - again first a short version to be displayed at the bottom;
%  - next the full list of authors, which may include contact information;
\author[Kun Li]{
  Kun Li \\\medskip
  {\small \url{kli@cise.ufl.edu}} \\ 
  {\small \url{http://www.cise.ufl.edu/~kli/}}}

% The institute:
%  - to start the name of the university as displayed on the top of each slide
%    this can be adjusted such that you can also create a Dutch version
%  - next the institute information as displayed on the title slide
\institute[University of Florida]{
  Department of Computer \& Information Science \& Engineering\\
  University of Florida}

% Add a date and possibly the name of the event to the slides
%  - again first a short version to be shown at the bottom of each slide
%  - second the full date and event name for the title slide
\date[Data Science Tea]{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}

  \tableofcontents
\end{frame}

% Section titles are shown in at the top of the slides with the current section 
% highlighted. Note that the number of sections determines the size of the top 
% bar, and hence the university name and logo. If you do not add any sections 
% they will not be visible.
\section{Introduction}

\begin{frame}
  \frametitle{Introduction}
  \begin{center}
  \includegraphics[height=10em, width=20em]{crf.jpg}\\
  \begin{tabular}{|l|c|c|}
  \multicolumn{3}{c}{\textbf{POS accuracy over different graphic models}} \\
  \hline
  \textcolor{white}{X} & error & oov error \\
  \hline
  HMM & $5.69\%$ & $45.99\%$ \\
  \hline
  MEMM & $6.37\%$ & $54.61\%$ \\
  \hline
  CRF & $5.55\%$ & $48.05\%$ \\
  \hline
  \hline
  \end{tabular}
  \label{tab:approaches}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Introduction}
  \begin{center}
  \includegraphics[height=10em]{models.jpg}\\
  \end{center}
  \begin{itemize}
    \item Conditional probability $p(y \mid x)$ rather than $p(y,x)$. Don't need to model distribution over variables, we don't care.
    \item Allow models with highly expressive features without worrying about wrong independencies.
  \end{itemize}
\end{frame}

\section{System Achitecture}

\begin{frame}
  \frametitle{System Achitecture}
  \begin{center}
  \includegraphics[height=18em]{system.png}
  \end{center}
\end{frame}


\section{Feature Extraction}
\begin{frame}
  \frametitle{Text Feature Extraction}
   \ Text feature extraction is a step in most statistical text analysis methods, and it can be an expensive operation. 
     \begin{itemize}
      \item dictionary features: does this token exist in a provided dictionary? 
      \item regex features: does this token match a provided regular expression? 
      \item edge features: is the label of a token correlated with the label of a previous token? 
      \item word features: does this the token appear in the training data? 
      \item position features: is this token the first or last in the token sequence? 
     \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Training Feature Extraction}
  \begin{itemize}
  %\item insert into temp\_feature(start\_pos, doc\_id, fname,pre\_label,label)\\ 
  \item insert into temp\_feature(start\_pos, doc\_id, f\_name, -1, label)\\ 
        select start\_pos, doc\_id, 'R\_' $\mid\mid$ name, -1, label\\
        from   textfex\_regex, textfex\_segmenttbl\\
        where  seg\_text $\thicksim$ pattern\\
  \item create sequence f\_seq start 1 increment 1;\\
        insert into feature\_index(f\_index,f\_type,pre\_label,label)\\ 
        select nextval('f\_seq') distinct on f\_type, prev\_label, label\\
        from   temp\_feature\\
  \item insert into feature(start\_pos, doc\_id, f\_name, f\_index)\\
        select start\_pos, doc\_id, temp\_feature.f\_name, f\_index\\
        from   temp\_feature, feature\_index\\
        where ...\\
  \item array aggregation in the ascending order of start\_pos, ascending order of f\_index.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Testing Feature Extraction}
  \begin{itemize}
  \item Use $M$ table to encode transision features \\
  $M$ schema: $mtbl(prev\_label,label,value)$.
  \includegraphics[height=7.02em]{m.png}\\
  \item Use $R$ table to encode single state features.\\ 
  $R$ schema: $rtbl(seg\_text,label,value)$. \\
  \includegraphics[height=5.85em]{r.png}\\
  \end{itemize}
\end{frame}

\section{CRF training and Viterbi Inference}
\begin{frame}
  \frametitle{CRFs Objective Function}
  %\textbf{\ Features}\vspace{-1.5mm}
  $\max p_\theta (y|x) = \frac{\exp(\sum_j{\lambda_jt_j(y_{i-1},y_i,x,i) + \sum_k{\mu_k s_k(y_i,x,i))}}}
                        {\sum_y {\exp(\sum_j{\lambda_jt_j(y_{i-1},y_i,x,i) + \sum_k{\mu_k s_k(y_i,x,i))}}}}$\\
  \begin{itemize}
  \item $\theta=(\lambda_1,\lambda_2,...;\mu_1,\mu_2,...)$
  \item $t_j(y_{i-1},y_i,x,i)$ is a transition feature function of the entire observation sequence and the labels at positions $i$ and 
       $i-1$ in the label sequence
  \item $s_k(y_i,x,i)$ is a state feature function of the label at position $i$ and the observation sequence
  \item $\lambda_i$ and $\mu_k$ are parameters to be estimated from training data.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Maximum Log\-likehood Principle}
  \begin{itemize}

 \item  Train a CRF by maximizing the log-likelihood of a give training set $T={(x_k,y_k)}_{k=1}^N$\\
 \item $\max \ell_{\lambda}=\sum_k{\log p_\lambda(y_k\mid x_k)}=\sum_k{(\lambda (F(y_k,x_k)-\log Z_\lambda(x_k))}$\\
 \item To perform this optimization, we seek the zero of the gradient\\
  $\nabla \ell_{\lambda}=\sum_k{(F(y_k,x_k)-E_{p_\lambda(Y|x_k)}F(Y,x_k))}$\\
 \item To avoid overfitting, penalize with prior\\
      $\max \ell_{\lambda}=\sum_k{(\lambda (F(y_k,x_k)-\log Z_\lambda(x_k))}-
\frac{\lVert \lambda \rVert_2}{2\sigma_2}
      +const$\\
  $\nabla \ell_{\lambda}=\sum_k{(F(y_k,x_k)-E_{p_\lambda(Y|x_k)}F(Y,x_k))}-\frac{\lVert \lambda \rVert}{\sigma_2}$\\
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Parallel Training CRFs in MADlib}
\begin{itemize}
\item Evaluate the log-likelihood function and gradient vector\\
$\ell_{\lambda_k}=\lambda F(y_k,x_k)-\log Z_\lambda(x_k)$(log-likelihood for doc. $k$)\\
$\nabla \ell_{\lambda_k}=\lambda F(y_k,x_k)-E_{p\lambda(Y|x_k)}F(Y,x_k)$(gradient vector for doc. $k$)\\
\item Sum up over all documents \\
$\ell_\lambda=\sum_k\ell_{\lambda_k}-\frac{\lambda^2}{2\sigma ^2}+const$(log-likelihood for all docs.)\\
$\nabla \ell_\lambda=\sum_k\nabla \ell_{\lambda_k}-\frac{\lambda}{\sigma ^2}$(gradient vector for all docs.)\\
\item Perform LBFGS optimization, a variation of quasi-Newton algorithm\\
$lbfgs(\lambda,\ell_\lambda,\nabla \ell_\lambda,Hessian,other args)$\\
\item Repeat step 1 and step2 until stop condition is satisfied
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Sequence Diagram for CRF training}
\begin{figure}
	\centering
	\includegraphics[scale=0.66]{LogisticRegression}
	\caption{Sequence Diagram for CRF}
	\label{fig:log-reg-driver}
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{User Defined Aggregates}
  \begin{itemize}
\item madlib.crf\_gradient\_step\_transition(state,otherargs)RETURNS state


\item madlib.crf\_gradient\_step\_merge\_states(state ,state) RETURNS state


\item madlib.crf\_gradient\_step\_final(state) RETURNS state
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Forward-backward algorithm}
  $E_{p\lambda(Y|x_k)}F(Y,x)$ can be computed efficiently using the forward-backward algorithm.
For a given x, define the transition matrix for position i as
$M_i[y,y']=\exp \lambda f(y,y',x,i)$\\
$E_{p\lambda(Y|x)}F(Y,x)=\sum_y{p_\lambda(y|x)F(y,x)}$
$=\sum_i{\frac{\alpha_{i-1}(f_i*M_i)\beta_i^T}{Z_\lambda(x)}}$\\
\begin{itemize}
\item $Z_\lambda(x)=\alpha_n \cdot 1^T$
\item $\alpha_i = 
       \begin{cases} \alpha_{i-1}M_i, &\text{if } i\ge0\\
                     1, & \text{if } i=0
       \end{cases}$
\item $
       \beta_i^T = 
       \begin{cases} M_{i+1}\beta_{i+1}^T, &\text{if } i\ge0\\
                     1, & \text{if } i=n
       \end{cases}
      $
\end{itemize}
Use a forward pass to compute the $\alpha_i$ and a backward pass to compute $\beta_i$ and accumulate the feature expectations.
\end{frame}

\begin{frame}
  \frametitle{The limited-memory BFGS}
  \begin{itemize}

  \item Newton Algorithm\\ 
  $f_T(x_n+\Delta x)=f_T(x)=f(x_n)+f'(x_n)\Delta x +1/2f''(x_n)\Delta^2$\\
  $f''(x_n)+f''(x_n)\Delta x=0;$
  $x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)},n=0,1,..$\\
  will converge towards a root of $f'(x^*)=0$
  \begin{center}
  \includegraphics[height=5em]{newton_grad.png}
  \end{center}
  \item LBFGS\\
 Limited-memory BFGS (L-BFGS) is a second-order
method that estimates the curvature numerically from
previous gradients and updates, avoiding the need for
an exact Hessian inverse computation.
  \end{itemize}
 
\end{frame}

%\section{Viterbi Inference}
\begin{frame}
  \frametitle{Viterbi Inference}
  The Viterbi algorithm is to find the top-k most likely labelings of a document 
for CRF models. 
We chose to implement a Python UDF that uses iterations to drive the Viterbi inference. 
In Greenplum, Viterbi can be run in parallel over different subsets 
of the document on a multi-core machine.
\[
V(i,y) =
\begin{cases}
 \max_{y'}(V(i-1,y')) + \textstyle \sum_{k=1}^K \lambda_kf_k(y,y',x_i), & \text{if } i\ge0 \\
 0, & \text{if } i=-1.
\end{cases}
\]
\begin{center}		             
  %\begin{multicols}{2}
      \includegraphics[height=7em,width=14em]{viterbip.jpg}
      \includegraphics[height=7em,width=14em]{result.png}\\
  %\end{multicols}
\end{center}
\end{frame}

\section{Conclusion and Future Work}

\begin{frame}
  \frametitle{Conclusion and Future Work}

  \begin{itemize}
    \item SELECT madlib.traindata\_textfex(args)(\textcolor{red}{ongoing})
    \item SELECT madlib.testdata\_textfex(args)
    \item SELECT madlib.linear\_crf\_training(args)(\textcolor{red}{ongoing})
    \item SELECT madlib.viterbi\_inference(args)
  \end{itemize}
  System: \textcolor{green}{Postgres} VS \textcolor{blue}{Greenplum(2 segments)}\\
	  Text Feature Extraction + Viterbi Inference
	  \includegraphics[height=9.9em]{extraction.png}
	  \includegraphics[height=9.9em]{viterbi.png}
\end{frame}

\begin{frame}
  \frametitle{Questions}
  \begin{center}
   Thanks! Questions?
  \end{center}
\end{frame}

\end{document}
