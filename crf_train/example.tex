\documentclass{beamer}

\usepackage[british]{babel}
\usepackage{graphicx,hyperref,ru,url}

% The title of the presentation:
%  - first a short version which is visible at the bottom of each slide;
%  - second the full title shown on the title slide;
\title[RU style for Beamer]{
  Linear-chain CRF for NLP in MADlib}

% The author(s) of the presentation:
%  - again first a short version to be displayed at the bottom;
%  - next the full list of authors, which may include contact information;
\author[Kun Li]{
  Kun Li \\\medskip
  {\small \url{kli@cise.ufl.edu}} \\ 
  {\small \url{http://www.cise.ufl.edu/~kli/}}}

% The institute:
%  - to start the name of the university as displayed on the top of each slide
%    this can be adjusted such that you can also create a Dutch version
%  - next the institute information as displayed on the title slide
\institute[University of Florida]{
  Department of Computer \& Information Science \& Engineering\\
  University of Florida}

% Add a date and possibly the name of the event to the slides
%  - again first a short version to be shown at the bottom of each slide
%  - second the full date and event name for the title slide
\date[slides Example 2010]{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}

  \tableofcontents
\end{frame}

% Section titles are shown in at the top of the slides with the current section 
% highlighted. Note that the number of sections determines the size of the top 
% bar, and hence the university name and logo. If you do not add any sections 
% they will not be visible.
\section{Introduction}

\begin{frame}
  \frametitle{Introduction}
  \begin{itemize}
    \item Conditional probability p(label sequence y|observation sequence x) rather than p(y,x)
    \item Allow arbitrary, non-independent features of the observation sequence X. 
    \item A CRF on (X,Y) is specified by a vector $f$ of local features and a corresponding weight vector
          lambda.
    \item Each local feature is either a $state feature s(y,x,i)$ or a transition feature t(y,y) 
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Introduction}
  \begin{center}
  \includegraphics[height=10em, width=20em]{crf.jpg}
  \end{center}
\end{frame}

\section{System Achitecture}

\begin{frame}
  \frametitle{System Achitecture}
  \begin{center}
  \includegraphics[height=18em]{system.png}
  \end{center}
\end{frame}


\section{Feature Extraction}
\begin{frame}
  \frametitle{Text Feature Extraction}
   \ Text feature extraction is a step in most statistical text analysis methods, and it can be an expensive operation. 
     To achieve high quality, CRF methods often assign tens of features to each token in the document. \\[1.5mm] 
     \textbf{\ Features}\vspace{-1.5mm}
     \begin{itemize}
      \item dictionary features: does this token exist in a provided dictionary? 
      \item regex features: does this token match a provided regular expression? 
      \item edge features: is the label of a token correlated with the label of a previous token? 
      \item word features: does this the token appear in the training data? 
      \item position features: is this token the first or last in the token sequence? 
     \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Training Feature Extraction}
\end{frame}


\begin{frame}
  \frametitle{Testing Feature Extraction}
  \begin{itemize}
  \item Use $M$ table to encode edge features, position features. \\
  $M$ schema: $mtbl(prev\_label,label,value)$.
  \item Use $R$ table to encode single state features such as dictionary features, regex features.\\ 
  $R$ schema: $rtbl(seg\_text,label,value)$. \\
  \end{itemize}
  \includegraphics[height=7.02em]{m.png}\\
  \includegraphics[height=5.85em]{r.png}\\
\end{frame}

\section{Linear-chain CRF training}

\begin{frame}
  \frametitle{Parallel Training CRFs in MADlib}
  \fbox{%
        \parbox{0.95\linewidth}{%
\textcolor{green}{\bf Parallel CRF training algorithm}\\
1.Evaluate the log-likelihood function and gradient vector\\
$\ell_{\lambda_k}=\lambda F(y_k,x_k)-\log Z_\lambda(x_k)$(log-likelihood for doc. $k$)\\
$\nabla \ell_{\lambda_k}=\lambda F(y_k,x_k)-E_{p\lambda(Y|x_k)}F(Y,x_k)$(gradient vector for doc. $k$)\\
2.Sum up over all documents \\
$\ell_\lambda=\sum_k\ell_{\lambda_k}-\frac{\lambda^2}{2\sigma ^2}+const$(log-likelihood for all docs.)\\
$\nabla \ell_\lambda=\sum_k\nabla \ell_{\lambda_k}-\frac{\lambda}{\sigma ^2}$(gradient vector for all docs.)\\
3.Perform LBFGS optimization, a variation of quasi-Newton algorithm\\
$lbfgs(\lambda,\ell_\lambda,\nabla \ell_\lambda,Hessian,other args)$\\
4.Repeat step 1 and step2 until stop condition is satisfied
        }%
}\\
\end{frame}

\begin{frame}
  \frametitle{Forward-backward algorithm}
  \begin{center}
  \includegraphics[height=18em]{system.png}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{The limited-memory BFGS}
  \begin{center}
  \includegraphics[height=18em]{system.png}
  \end{center}
\end{frame}

\section{Viterbi Inference}
\begin{frame}
  \frametitle{Viterbi Inference}
  The Viterbi algorithm is the popular algorithm to find the top-k most likely labelings of a document 
for CRF models. 
We chose to implement a Python UDF that uses iterations to drive the Viterbi inference. Each iteration will 
finish the inference over one document.
In Greenplum, Viterbi can be run in parallel over different subsets 
of the document on a multi-core machine.
\[
V(i,y) =
\begin{cases}
 \max_{y^\prime}(V(i-1,y^\prime)) + \textstyle \sum_{k=1}^K \lambda_kf_k(y,y^\prime,x_i), & \text{if } i\ge0 \\
 0, & \text{if } i=-1.
\end{cases}
\]
\begin{center}		             
  %\begin{multicols}{2}
      \includegraphics[height=5em,width=11em]{viterbip.jpg}
      \includegraphics[height=5em,width=11em]{result.png}\\
  %\end{multicols}
\end{center}
\end{frame}

\section{Conclusion and Future Work}

\begin{frame}
  \frametitle{Conclusion and Future Work}

  \begin{itemize}
    \item SELECT madlib.traindata\_textfex(args)
    \item SELECT madlib.testdata\_textfex(args)
    \item SELECT madlib.linear\_crf\_training(args)
    \item SELECT madlib.viterbi\_inference(args)
  \end{itemize}
  \begin{center}System: \textcolor{green}{Postgres} VS \textcolor{blue}{Greenplum(2 segments)}\\
	  Text Feature Extraction + Viterbi Inference\end{center}
  \begin{center}
	  \includegraphics[height=9.9em]{extraction.png}
	  \includegraphics[height=9.9em]{viterbi.png}
  \end{center}
\end{frame}

\end{document}
